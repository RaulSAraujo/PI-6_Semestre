{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "0adb9499",
            "metadata": {},
            "source": [
                "\n",
                "# Classificação de Direção de Ações Brasileiras com Machine Learning\n",
                "\n",
                "Este notebook apresenta um pipeline completo de classificação para prever a direção da variação percentual diária do preço de fechamento de ações brasileiras. Utilizaremos dados históricos de cinco ativos: BBAS3, EMBR3, TOTS3, PETR4 e KLBN3. O objetivo é construir e comparar modelos capazes de prever se a variação do próximo dia será positiva (alta) ou negativa (baixa).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7ad23803",
            "metadata": {},
            "source": [
                "\n",
                "## 1. Carregamento e Limpeza dos Dados\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dfe4c99f",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import pandas as pd\n",
                "\n",
                "tickers_files = {\n",
                "    'BBAS3': 'BBAS3_Dados_Históricos.csv',\n",
                "    'EMBR3': 'EMBR3_Dados_Históricos.csv',\n",
                "    'TOTS3': 'TOTS3_Dados_Históricos.csv',\n",
                "    'PETR4': 'PETR4_Dados_Históricos.csv',\n",
                "    'KLBN3': 'KLBN3_Dados_Históricos.csv'\n",
                "}\n",
                "\n",
                "def load_and_clean(file, ticker):\n",
                "    df = pd.read_csv(file, sep='|')\n",
                "    df.columns = [col.strip() for col in df.columns]\n",
                "    data_col = [col for col in df.columns if 'data' in col.lower()][0]\n",
                "    df['Data'] = pd.to_datetime(df[data_col], format='%d.%m.%Y')\n",
                "    for col in ['Último', 'Abertura', 'Máxima', 'Mínima']:\n",
                "        col_found = [c for c in df.columns if col.lower() in c.lower()]\n",
                "        if col_found:\n",
                "            df[col] = df[col_found[0]].astype(str).str.replace('.', '', regex=False).str.replace(',', '.', regex=False).astype(float)\n",
                "    vol_col = [c for c in df.columns if 'vol' in c.lower()][0]\n",
                "    df['Vol.'] = df[vol_col].astype(str).str.replace('.', '', regex=False).str.replace('K', 'e3').str.replace('M', 'e6').apply(pd.eval)\n",
                "    var_col = [c for c in df.columns if 'var' in c.lower()][0]\n",
                "    df['Var%'] = df[var_col].astype(str).str.replace('%', '', regex=False).str.replace(',', '.', regex=False).astype(float)\n",
                "    df['Ticker'] = ticker\n",
                "    return df[['Data','Último','Abertura','Máxima','Mínima','Vol.','Var%','Ticker']]\n",
                "\n",
                "dfs = []\n",
                "for ticker, file in tickers_files.items():\n",
                "    dfs.append(load_and_clean(file, ticker))\n",
                "data = pd.concat(dfs, ignore_index=True)\n",
                "data = data.sort_values(['Ticker', 'Data'])\n",
                "data.head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2475f757",
            "metadata": {},
            "source": [
                "\n",
                "## 2. Análise Exploratória dos Dados (EDA)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8d27cedd",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "display(data.describe())\n",
                "\n",
                "plt.figure(figsize=(12,6))\n",
                "sns.histplot(data['Var%'], bins=50, kde=True)\n",
                "plt.title('Distribuição da Variação Percentual Diária')\n",
                "plt.show()\n",
                "\n",
                "plt.figure(figsize=(14,7))\n",
                "for ticker in tickers_files.keys():\n",
                "    subset = data[data['Ticker'] == ticker]\n",
                "    plt.plot(subset['Data'], subset['Último'], label=ticker)\n",
                "plt.legend()\n",
                "plt.title('Preço de Fechamento ao Longo do Tempo')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d8612fd8",
            "metadata": {},
            "source": [
                "\n",
                "## 3. Pré-processamento e Feature Engineering\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "355c3cdb",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def add_lags(df, n_lags=3):\n",
                "    for lag in range(1, n_lags+1):\n",
                "        df[f'Var%_lag{lag}'] = df['Var%'].shift(lag)\n",
                "        df[f'Último_lag{lag}'] = df['Último'].shift(lag)\n",
                "    return df\n",
                "\n",
                "data = data.groupby('Ticker').apply(add_lags).reset_index(drop=True)\n",
                "data = data.dropna()\n",
                "\n",
                "# Criar coluna de target binária: 1 para alta, 0 para baixa\n",
                "# (considerando 0 como neutro, pode ser removido se preferir)\n",
                "data['Target'] = (data['Var%'] > 0).astype(int)\n",
                "\n",
                "features = [col for col in data.columns if 'lag' in col]\n",
                "target = 'Target'\n",
                "data[features + [target]].head()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "05391ea9",
            "metadata": {},
            "source": [
                "\n",
                "## 4. Divisão de Dados em Treino e Teste\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6c306eb8",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "def split_train_test(df, frac=0.8):\n",
                "    idx = int(len(df)*frac)\n",
                "    return df.iloc[:idx], df.iloc[idx:]\n",
                "\n",
                "train = []\n",
                "test = []\n",
                "for ticker in tickers_files.keys():\n",
                "    t, s = split_train_test(data[data['Ticker'] == ticker])\n",
                "    train.append(t)\n",
                "    test.append(s)\n",
                "train = pd.concat(train)\n",
                "test = pd.concat(test)\n",
                "\n",
                "X_train = train[features]\n",
                "y_train = train[target]\n",
                "X_test = test[features]\n",
                "y_test = test[target]\n",
                "\n",
                "print('Exemplo de X_train:')\n",
                "display(X_train.head())\n",
                "print('Distribuição do target no treino:')\n",
                "print(y_train.value_counts(normalize=True))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "bd9f203f",
            "metadata": {},
            "source": [
                "\n",
                "## 5. Treinamento e Avaliação de Modelos\n",
                "\n",
                "Vamos comparar Random Forest, Regressão Logística e SVM.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "02f25f91",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Random Forest\n",
                "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "rf.fit(X_train, y_train)\n",
                "y_pred_rf = rf.predict(X_test)\n",
                "print('Random Forest:')\n",
                "print(classification_report(y_test, y_pred_rf))\n",
                "print('Matriz de confusão:')\n",
                "print(confusion_matrix(y_test, y_pred_rf))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c44ef25d",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# Regressão Logística\n",
                "lr = LogisticRegression(max_iter=1000)\n",
                "lr.fit(X_train, y_train)\n",
                "y_pred_lr = lr.predict(X_test)\n",
                "print('Logistic Regression:')\n",
                "print(classification_report(y_test, y_pred_lr))\n",
                "print('Matriz de confusão:')\n",
                "print(confusion_matrix(y_test, y_pred_lr))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "14373fc7",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "# SVM\n",
                "svc = SVC(kernel='rbf', probability=True)\n",
                "svc.fit(X_train, y_train)\n",
                "y_pred_svc = svc.predict(X_test)\n",
                "print('SVM:')\n",
                "print(classification_report(y_test, y_pred_svc))\n",
                "print('Matriz de confusão:')\n",
                "print(confusion_matrix(y_test, y_pred_svc))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c802dcb9",
            "metadata": {},
            "source": [
                "\n",
                "## 6. Comparação dos Modelos\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2ccd2596",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "results = pd.DataFrame({\n",
                "    'Random Forest': [accuracy_score(y_test, y_pred_rf)],\n",
                "    'Logistic Regression': [accuracy_score(y_test, y_pred_lr)],\n",
                "    'SVM': [accuracy_score(y_test, y_pred_svc)]\n",
                "}, index=['Acurácia'])\n",
                "display(results)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9731374e",
            "metadata": {},
            "source": [
                "\n",
                "## 7. Importância das Features (Random Forest)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b9109ca",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "importances = rf.feature_importances_\n",
                "feat_imp = pd.Series(importances, index=features).sort_values(ascending=False)\n",
                "feat_imp.plot(kind='bar', figsize=(12,5), title='Importância das Features')\n",
                "plt.show()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9be4f9e3",
            "metadata": {},
            "source": [
                "\n",
                "## 8. Salvando o Modelo Treinado\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2bb65043",
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import pickle\n",
                "with open('model.pkl', 'wb') as f:\n",
                "    pickle.dump(rf, f)\n",
                "print('Modelo salvo como model.pkl')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8fd6ec68",
            "metadata": {},
            "source": [
                "\n",
                "## 9. Conclusão e Próximos Passos\n",
                "\n",
                "Neste notebook, realizamos a classificação da direção da variação percentual diária de ações brasileiras, incluindo:\n",
                "\n",
                "- Carregamento e limpeza robusta dos dados históricos.\n",
                "- Análise exploratória e visualização dos dados.\n",
                "- Feature engineering com lags e criação de target binário.\n",
                "- Treinamento e avaliação de três modelos: Random Forest, Regressão Logística e SVM.\n",
                "- Comparação quantitativa dos resultados.\n",
                "- Análise de importância das features.\n",
                "- Salvamento do modelo treinado para uso futuro.\n",
                "\n",
                "**Próximos passos:**\n",
                "- Testar outros algoritmos (ex: XGBoost, LightGBM).\n",
                "- Realizar tuning de hiperparâmetros.\n",
                "- Incluir mais features (ex: indicadores técnicos).\n",
                "- Avaliar o modelo em dados mais recentes ou em produção.\n",
                "\n",
                "*Este notebook serve como base para experimentação e pode ser expandido conforme a necessidade do projeto.*\n"
            ]
        }
    ],
    "metadata": {},
    "nbformat": 4,
    "nbformat_minor": 5
}